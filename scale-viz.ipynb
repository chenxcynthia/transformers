{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5f650f-8f49-421b-95bd-e0c08d8b25f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports + Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48115fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import methods from bertviz\n",
    "from bertviz import neuron_view\n",
    "from bertviz.transformers_neuron_view import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from numpy import linalg as LA\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee6af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = np.load('sentences.npy') # load sentences from Catherine's file\n",
    "# sentences_test = sentences[:10] # small sample to test out code with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f47d1",
   "metadata": {},
   "source": [
    "### Select BERT or GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9142dbe-1fe2-4a9e-a9ea-ba9899362552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # BERT\n",
    "# model_type = 'bert'\n",
    "# model_version = 'bert-base-uncased'\n",
    "# model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=True)\n",
    "\n",
    "# GPT\n",
    "model_type = 'gpt2'\n",
    "model_version = 'gpt2'\n",
    "model = GPT2Model.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_version, do_lower_case=True)\n",
    "\n",
    "num_heads = 12\n",
    "num_layers = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c480a791",
   "metadata": {},
   "source": [
    "### Computing Q/K norm ratios per attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "966f7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 12 x 12 array for query/key norm ratios\n",
    "\n",
    "attn_norms = []\n",
    "for i in range(num_heads):\n",
    "    curr = []\n",
    "    for j in range(num_layers):\n",
    "        curr_dict = {}\n",
    "        curr_dict['key_norm_sum'] = 0\n",
    "        curr_dict['query_norm_sum'] = 0\n",
    "        curr.append(curr_dict)\n",
    "    attn_norms.append(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24817ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    # call method from bertviz to get attention info\n",
    "    s_dict = neuron_view.get_attention(model, model_type, tokenizer, s, include_queries_and_keys=True)['all']\n",
    "    \n",
    "    # append to master dictionary\n",
    "    tokens = s_dict['left_text']\n",
    "        \n",
    "    # find q/k ratios per attention head\n",
    "    for i in range(num_heads):\n",
    "        for j in range(num_layers):\n",
    "            query = s_dict['queries'][i][j]\n",
    "            for q in query:\n",
    "                attn_norms[i][j]['query_norm_sum'] += LA.norm(q)\n",
    "            \n",
    "            key = s_dict['keys'][i][j]\n",
    "            for k in key:\n",
    "                attn_norms[i][j]['key_norm_sum'] += LA.norm(k)\n",
    "                \n",
    "            attn_norms[i][j]['qk_factor'] = attn_norms[i][j]['query_norm_sum'] / attn_norms[i][j]['key_norm_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d0e8308",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = np.zeros([12, 12])\n",
    "for i in range(num_heads):\n",
    "    for j in range(num_layers):\n",
    "        factors[i][j] = attn_norms[i][j]['qk_factor'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226e431",
   "metadata": {},
   "source": [
    "### Visualize ratios using heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15232cfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6), dpi = 150)\n",
    "s = sns.heatmap(factors,fmt=\"\",cmap='RdYlGn',linewidths=0.50,ax=ax)\n",
    "s.set_xlabel('Layer', fontsize=13)\n",
    "s.set_ylabel('Head', fontsize=13)\n",
    "plt.title('Average Q/K Norm Ratio Per Attention Head', fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38559c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2670928274360302"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9516e8d8",
   "metadata": {},
   "source": [
    "### Scaling queries + keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7955922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # master dictionary for all values\n",
    "# attn_dict = {'left_text': [], \n",
    "#              'right_text': [], \n",
    "#              'positions': [],\n",
    "#              'normalized_positions': [],\n",
    "#              'sentences': [],\n",
    "#              'tokenized_sentences': [],\n",
    "#              'queries': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)], \n",
    "#              'keys': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)],\n",
    "#              'attn': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)],\n",
    "#              'dot_prod': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95f81c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in sentences[0:100]:\n",
    "#     # call method from bertviz to get attention info\n",
    "#     s_dict = neuron_view.get_attention(model, model_type, tokenizer, s, include_queries_and_keys=True)['all']\n",
    "    \n",
    "#     # append to master dictionary\n",
    "#     tokens = s_dict['left_text']\n",
    "#     attn_dict['left_text'].extend(tokens)\n",
    "#     attn_dict['right_text'].extend(s_dict['right_text'])\n",
    "    \n",
    "    \n",
    "#     for index in range(len(tokens)): # save position of token and tokenized sentences too\n",
    "#         attn_dict['positions'].append(index)\n",
    "#         attn_dict['normalized_positions'].append(index / (len(tokens) - 1))\n",
    "#         attn_dict['sentences'].append(s)\n",
    "#         attn_dict['tokenized_sentences'].append(' '.join(tokens))\n",
    "#         attn_dict_scaled['positions'].append(index)\n",
    "#         attn_dict_scaled['normalized_positions'].append(index / (len(tokens) - 1))\n",
    "#         attn_dict_scaled['sentences'].append(s)\n",
    "#         attn_dict_scaled['tokenized_sentences'].append(' '.join(tokens))\n",
    "        \n",
    "# #     for i in range(num_heads): # updating cumulative q/k vectors + attn + dp\n",
    "# #         for j in range(num_layers):\n",
    "#     i = 1\n",
    "#     j = 2\n",
    "#     k = attn_dict['keys'][i][j]\n",
    "#     a = attn_dict['attn'][i][j]\n",
    "#     d = attn_dict['dot_prod'][i][j]\n",
    "\n",
    "#     if len(q) == 1: # on first round, need to empty list (random string was placeholder)\n",
    "#         q.clear()\n",
    "#     query = s_dict['queries'][i][j]\n",
    "#     q.extend(query)\n",
    "#     np_query = np.array(query)\n",
    "\n",
    "#     if len(k) == 1:\n",
    "#         k.clear()\n",
    "#     key = s_dict['keys'][i][j]\n",
    "#     k.extend(key)\n",
    "#     np_key = np.array(key)\n",
    "\n",
    "#     if len(a) == 1:\n",
    "#         a.clear()\n",
    "#     a.extend(s_dict['attn'][i][j])\n",
    "\n",
    "#     if len(d) == 1:\n",
    "#         d.clear()\n",
    "#     dp = np.dot(np_query, np_key.transpose())\n",
    "#     d.extend(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "37554b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # master dictionary for all values\n",
    "# attn_dict_scaled = {'left_text': [], \n",
    "#              'right_text': [], \n",
    "#              'positions': [],\n",
    "#              'normalized_positions': [],\n",
    "#              'sentences': [],\n",
    "#              'tokenized_sentences': [],\n",
    "#              'queries': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)], \n",
    "#              'keys': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)],\n",
    "#              'attn': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)],\n",
    "#              'dot_prod': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c14863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling_factor = factors[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37147139-7f13-4cd8-8f15-990560a39925",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make TSNE / UMAP Plots\n",
    "Generating plots from query + key vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221343d8-5f39-436b-9bc3-cd2647214bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly\n",
    "# !pip install seaborn\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8520c4d-7dd9-4fdc-9b03-94d9174314b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7799895a-ef95-4730-89ca-e1805f748ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ensure plots show up in jupyter\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec11597-843c-442b-874d-e773909a500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load attn_dict back if pre-saved\n",
    "\n",
    "attn_dict = pickle.load(open(\"saved/attn_dict.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "768b6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_dict_small = pickle.load(open(\"saved/attn_dict_small.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9215e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28679"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attn_dict['queries'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f37d1a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2883"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attn_dict_small['queries'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bea1ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "# df['token'] = attn_dict['left_text'] + attn_dict['right_text'] # store tokens\n",
    "# df['token'] = df['token'].str.lower() # convert to lowercase\n",
    "# num_tokens = len(attn_dict['left_text'])\n",
    "\n",
    "# df['type'] = ['query'] * num_tokens + ['key'] * num_tokens # store token type\n",
    "# df['pos_int'] = attn_dict['positions'] * 2 # positions\n",
    "# df['position'] = attn_dict['normalized_positions'] * 2\n",
    "\n",
    "# # sentence itself\n",
    "# df['sentence'] = fix_sentences(attn_dict['tokenized_sentences'], attn_dict['positions'], df['type'][:num_tokens]) + fix_sentences(attn_dict['tokenized_sentences'], attn_dict['positions'], df['type'][num_tokens:])\n",
    "\n",
    "# # save attn info\n",
    "# attn = attn_dict['attn'][layer][head]\n",
    "# df['attn'] = attn + k_matrix(attn)\n",
    "# dp = attn_dict['dot_prod'][layer][head]\n",
    "# df['dot_prod'] = dp + k_matrix(dp)\n",
    "\n",
    "# # extract q/k vectors\n",
    "# queries = attn_dict['queries']\n",
    "# keys = attn_dict['keys']\n",
    "# vec_size = len(queries[layer][head][0])\n",
    "\n",
    "# # norms\n",
    "# norms_q = []\n",
    "# norms_k = []\n",
    "# for i in range(len(queries[layer][head])):\n",
    "#     q = queries[layer][head][i]\n",
    "#     k = keys[layer][head][i]\n",
    "#     norms_q.append(np.linalg.norm(q))\n",
    "#     norms_k.append(np.linalg.norm(k))\n",
    "# df[\"norm\"] = norms_q + norms_k\n",
    "\n",
    "# for i in range(vec_size): # store q/k vector values\n",
    "#     qs = [queries[layer][head][j][i] for j in range(num_tokens)]\n",
    "#     ks = [keys[layer][head][j][i] for j in range(num_tokens)]\n",
    "#     df[\"f\" + str(i)] = qs + ks # add to dataframe\n",
    "\n",
    "# # comment out line below if want all 60k data points\n",
    "# df = pd.concat([df.iloc[:5021], df.iloc[30070:30070+5021]]) # only get first X keys + queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "849b47c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>type</th>\n",
       "      <th>pos_int</th>\n",
       "      <th>position</th>\n",
       "      <th>sentence</th>\n",
       "      <th>attn</th>\n",
       "      <th>dot_prod</th>\n",
       "      <th>norm</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>...</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>f58</th>\n",
       "      <th>f59</th>\n",
       "      <th>f60</th>\n",
       "      <th>f61</th>\n",
       "      <th>f62</th>\n",
       "      <th>f63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>syn</td>\n",
       "      <td>query</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>&lt;b style='color:#B6E1B9'&gt;Syn&lt;/b&gt; th pop band F...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-1.4260597666596735, -14.045386268408585, -9....</td>\n",
       "      <td>4.776521</td>\n",
       "      <td>0.626356</td>\n",
       "      <td>-0.402041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012074</td>\n",
       "      <td>-0.468083</td>\n",
       "      <td>0.165571</td>\n",
       "      <td>-1.149125</td>\n",
       "      <td>-1.470522</td>\n",
       "      <td>-0.722273</td>\n",
       "      <td>0.825755</td>\n",
       "      <td>-0.582111</td>\n",
       "      <td>0.639374</td>\n",
       "      <td>-0.158392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>th</td>\n",
       "      <td>query</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>Syn &lt;b style='color:#B6E1B9'&gt;th&lt;/b&gt; pop band F...</td>\n",
       "      <td>[0.7727242708206177, 0.2272757887840271, 0.0, ...</td>\n",
       "      <td>[-2.37188420581595, -12.161947897568501, -14.2...</td>\n",
       "      <td>5.630867</td>\n",
       "      <td>1.253516</td>\n",
       "      <td>0.493161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.952495</td>\n",
       "      <td>-1.166787</td>\n",
       "      <td>0.505097</td>\n",
       "      <td>-0.813122</td>\n",
       "      <td>-0.057576</td>\n",
       "      <td>-1.366027</td>\n",
       "      <td>0.561580</td>\n",
       "      <td>-0.465369</td>\n",
       "      <td>1.410642</td>\n",
       "      <td>0.122727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pop</td>\n",
       "      <td>query</td>\n",
       "      <td>2</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>Syn th &lt;b style='color:#B6E1B9'&gt;pop&lt;/b&gt; band F...</td>\n",
       "      <td>[0.6718084216117859, 0.28001973032951355, 0.04...</td>\n",
       "      <td>[-0.7664155974118483, -7.767320269121932, -21....</td>\n",
       "      <td>8.130524</td>\n",
       "      <td>0.393987</td>\n",
       "      <td>1.333136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.364882</td>\n",
       "      <td>-3.260059</td>\n",
       "      <td>-1.110516</td>\n",
       "      <td>-0.283682</td>\n",
       "      <td>-2.054488</td>\n",
       "      <td>-0.639621</td>\n",
       "      <td>0.065158</td>\n",
       "      <td>-0.128336</td>\n",
       "      <td>1.627604</td>\n",
       "      <td>-0.476792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>band</td>\n",
       "      <td>query</td>\n",
       "      <td>3</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>Syn th pop &lt;b style='color:#B6E1B9'&gt;band&lt;/b&gt; F...</td>\n",
       "      <td>[0.6222391128540039, 0.07751425355672836, 0.11...</td>\n",
       "      <td>[3.0192838275094527, -13.643618085799897, -10....</td>\n",
       "      <td>7.561337</td>\n",
       "      <td>0.215017</td>\n",
       "      <td>-0.998050</td>\n",
       "      <td>...</td>\n",
       "      <td>1.107949</td>\n",
       "      <td>-2.849341</td>\n",
       "      <td>-0.212997</td>\n",
       "      <td>0.122969</td>\n",
       "      <td>-1.968271</td>\n",
       "      <td>0.098098</td>\n",
       "      <td>-0.118798</td>\n",
       "      <td>-1.305930</td>\n",
       "      <td>0.840051</td>\n",
       "      <td>1.869990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>free</td>\n",
       "      <td>query</td>\n",
       "      <td>4</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>Syn th pop band &lt;b style='color:#B6E1B9'&gt;Free&lt;...</td>\n",
       "      <td>[0.29884660243988037, 0.14810486137866974, 0.0...</td>\n",
       "      <td>[-3.873843952510504, -9.489924005559253, -17.6...</td>\n",
       "      <td>6.836336</td>\n",
       "      <td>0.908945</td>\n",
       "      <td>-0.601588</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.241019</td>\n",
       "      <td>-0.022176</td>\n",
       "      <td>0.152536</td>\n",
       "      <td>-0.391612</td>\n",
       "      <td>0.282707</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.233563</td>\n",
       "      <td>0.197837</td>\n",
       "      <td>0.379938</td>\n",
       "      <td>0.536506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   token   type  pos_int  position  \\\n",
       "0    syn  query        0  0.000000   \n",
       "1     th  query        1  0.076923   \n",
       "2    pop  query        2  0.153846   \n",
       "3   band  query        3  0.230769   \n",
       "4   free  query        4  0.307692   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  <b style='color:#B6E1B9'>Syn</b> th pop band F...   \n",
       "1  Syn <b style='color:#B6E1B9'>th</b> pop band F...   \n",
       "2  Syn th <b style='color:#B6E1B9'>pop</b> band F...   \n",
       "3  Syn th pop <b style='color:#B6E1B9'>band</b> F...   \n",
       "4  Syn th pop band <b style='color:#B6E1B9'>Free<...   \n",
       "\n",
       "                                                attn  \\\n",
       "0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.7727242708206177, 0.2272757887840271, 0.0, ...   \n",
       "2  [0.6718084216117859, 0.28001973032951355, 0.04...   \n",
       "3  [0.6222391128540039, 0.07751425355672836, 0.11...   \n",
       "4  [0.29884660243988037, 0.14810486137866974, 0.0...   \n",
       "\n",
       "                                            dot_prod      norm        f0  \\\n",
       "0  [-1.4260597666596735, -14.045386268408585, -9....  4.776521  0.626356   \n",
       "1  [-2.37188420581595, -12.161947897568501, -14.2...  5.630867  1.253516   \n",
       "2  [-0.7664155974118483, -7.767320269121932, -21....  8.130524  0.393987   \n",
       "3  [3.0192838275094527, -13.643618085799897, -10....  7.561337  0.215017   \n",
       "4  [-3.873843952510504, -9.489924005559253, -17.6...  6.836336  0.908945   \n",
       "\n",
       "         f1  ...       f54       f55       f56       f57       f58       f59  \\\n",
       "0 -0.402041  ...  0.012074 -0.468083  0.165571 -1.149125 -1.470522 -0.722273   \n",
       "1  0.493161  ... -0.952495 -1.166787  0.505097 -0.813122 -0.057576 -1.366027   \n",
       "2  1.333136  ... -0.364882 -3.260059 -1.110516 -0.283682 -2.054488 -0.639621   \n",
       "3 -0.998050  ...  1.107949 -2.849341 -0.212997  0.122969 -1.968271  0.098098   \n",
       "4 -0.601588  ... -1.241019 -0.022176  0.152536 -0.391612  0.282707  0.000389   \n",
       "\n",
       "        f60       f61       f62       f63  \n",
       "0  0.825755 -0.582111  0.639374 -0.158392  \n",
       "1  0.561580 -0.465369  1.410642  0.122727  \n",
       "2  0.065158 -0.128336  1.627604 -0.476792  \n",
       "3 -0.118798 -1.305930  0.840051  1.869990  \n",
       "4  0.233563  0.197837  0.379938  0.536506  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca9df7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20745466649532318,\n",
       " 0.05711399018764496,\n",
       " 0.21443572640419006,\n",
       " 0.10953076928853989,\n",
       " 0.018751924857497215,\n",
       " 0.06923841685056686,\n",
       " 0.04730606824159622,\n",
       " 0.0810045525431633,\n",
       " 0.054247643798589706,\n",
       " 0.09515170753002167,\n",
       " 0.04576465114951134,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[10]['attn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0feda50-9cd5-434d-9722-3b1f24e64475",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Visualization helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36034c12-503c-4eef-80a9-1730035feb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce corresponding key matrix from query matrix (e.g., for attention)\n",
    "def k_matrix(q_matrix):\n",
    "    # assumes for specific layer + head (e.g., queries[0][0])\n",
    "    num_tokens = len(q_matrix)\n",
    "    k_matrix = []\n",
    "    i = 0\n",
    "    while i < num_tokens:\n",
    "        q = q_matrix[i]\n",
    "        sent_length = len(q)\n",
    "        for k_i in range(sent_length):\n",
    "            k = []\n",
    "            for q_i in range(sent_length):\n",
    "                k.append(q_matrix[q_i + i][k_i])\n",
    "            k_matrix.append(k)\n",
    "        i += sent_length\n",
    "    \n",
    "    return k_matrix\n",
    "\n",
    "# format sentences to be displayed in html plot\n",
    "def fix_sentences(sentences, positions, types):\n",
    "    new_sentences = []\n",
    "    for sent, pos, t in zip(sentences, positions, types):\n",
    "        s_arr = sent.split()\n",
    "        s = \"\"\n",
    "        for i in range(len(s_arr)):\n",
    "            if i % 10 == 0 and i not in [0, len(s_arr) - 1]:\n",
    "                s += \"<br>\" # add new line every 10 tokens\n",
    "                \n",
    "            if i == pos: # italicize  + color current token\n",
    "                color = \"#B6E1B9\"\n",
    "                if t == \"key\":\n",
    "                    color = \"#F6BA98\"\n",
    "                s += \"<b style='color:\" + color + \"'>\" + s_arr[i] + \"</b>\"\n",
    "            else:\n",
    "                s += s_arr[i]\n",
    "                \n",
    "            if s != len(s_arr) - 1:\n",
    "                s += \" \" # add space back between each token\n",
    "        new_sentences.append(s)\n",
    "    \n",
    "    return new_sentences\n",
    "\n",
    "# convert data into pandas dataframe\n",
    "def make_df(layer, head, attn_dict, scale = 1):\n",
    "    df = pd.DataFrame()\n",
    "    df['token'] = attn_dict['left_text'] + attn_dict['right_text'] # store tokens\n",
    "    df['token'] = df['token'].str.lower() # convert to lowercase\n",
    "    num_tokens = len(attn_dict['left_text'])\n",
    "    \n",
    "    df['type'] = ['query'] * num_tokens + ['key'] * num_tokens # store token type\n",
    "    df['pos_int'] = attn_dict['positions'] * 2 # positions\n",
    "    df['position'] = attn_dict['normalized_positions'] * 2\n",
    "    \n",
    "    # sentence itself\n",
    "    df['sentence'] = fix_sentences(attn_dict['tokenized_sentences'], attn_dict['positions'], df['type'][:num_tokens]) + fix_sentences(attn_dict['tokenized_sentences'], attn_dict['positions'], df['type'][num_tokens:])\n",
    "\n",
    "    # save attn info\n",
    "    attn = attn_dict['attn'][layer][head]\n",
    "    df['attn'] = attn + k_matrix(attn)\n",
    "    dp = attn_dict['dot_prod'][layer][head]\n",
    "    df['dot_prod'] = dp + k_matrix(dp)\n",
    "    \n",
    "    # extract q/k vectors\n",
    "    queries = attn_dict['queries']\n",
    "    keys = attn_dict['keys']\n",
    "    vec_size = len(queries[layer][head][0])\n",
    "    \n",
    "    # norms\n",
    "    norms_q = []\n",
    "    norms_k = []\n",
    "    for i in range(len(queries[layer][head])):\n",
    "        q = queries[layer][head][i]\n",
    "        k = keys[layer][head][i]\n",
    "        norms_q.append(np.linalg.norm(q))\n",
    "        norms_k.append(np.linalg.norm(k))\n",
    "    df[\"norm\"] = norms_q + norms_k\n",
    "\n",
    "    for i in range(vec_size): # store q/k vector values\n",
    "        qs = [queries[layer][head][j][i]/scale for j in range(num_tokens)]\n",
    "        ks = [keys[layer][head][j][i]*scale for j in range(num_tokens)]\n",
    "        df[\"f\" + str(i)] = qs + ks # add to dataframe\n",
    "        \n",
    "    # comment out line below if want all 60k data points\n",
    "    df = pd.concat([df.iloc[:5021], df.iloc[30070:30070+5021]]) # only get first X keys + queries\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c9984a-cf26-4da2-bb4c-3183d41c22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRANSLATING KEYS FOR EASIER COMPARISON\n",
    "def find_q_means(df):\n",
    "    # find mean of each feature in query embeddings\n",
    "    df_queries = df.loc[df['type'] == 'query']\n",
    "    df_queries = df_queries.iloc[:, 8:].copy()\n",
    "    query_means = df_queries.mean(axis=0)\n",
    "    return query_means\n",
    "\n",
    "def find_k_means(df):\n",
    "    # find mean of each feature in key embeddings\n",
    "    df_keys = df.loc[df['type'] == 'key']\n",
    "    df_keys = df_keys.iloc[:, 8:].copy()\n",
    "    key_means = df_keys.mean(axis=0)\n",
    "    return df_keys, key_means\n",
    "\n",
    "def translate_keys(df, df_keys, query_means, key_means):\n",
    "    # translate key vectors accordingly\n",
    "    for i in range(64):\n",
    "        col = \"f\" + str(i)\n",
    "        new_key = df_keys[col] - key_means[col] + query_means[col]\n",
    "        df.loc[df['type'] == 'key', col] = new_key\n",
    "    return df\n",
    "\n",
    "def translate_loop(df): \n",
    "    # whole translation loop\n",
    "    query_means = find_q_means(df)\n",
    "    df_keys, key_means = find_k_means(df)\n",
    "    df = translate_keys(df, df_keys, query_means, key_means)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd4e2102-fdfe-4588-a31f-c7274652b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TSNE AND UMAP\n",
    "def run_tsne(df, layer, head): \n",
    "    # prepare data for feature plot\n",
    "    df_sub = df.iloc[:, 8:].copy()\n",
    "    df_subset = df_sub.values # only get feature cols\n",
    "    \n",
    "    # run TSNE\n",
    "    # from: https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n",
    "    time_start = time.time()\n",
    "    tsne = TSNE(n_components=3, verbose=0, perplexity=100, n_iter=300, metric=\"cosine\") # 3D\n",
    "    # tsne = TSNE(n_components=2, verbose=0, perplexity=100, n_iter=300, metric=\"cosine\") # 2D\n",
    "    tsne_results = tsne.fit_transform(df_subset)\n",
    "    # np.save(\"tsne/layer\" + str(layer) + \"_head\" + str(head) + \".npy\", tsne_results) # save tsne results too\n",
    "    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "    \n",
    "    return tsne_results\n",
    "\n",
    "def run_umap(df, layer, head):\n",
    "    # prepare data for feature plot\n",
    "    df_sub = df.iloc[:, 8:].copy()\n",
    "    df_subset = df_sub.values # only get feature cols\n",
    "    \n",
    "    # run umap\n",
    "    time_start = time.time()\n",
    "    umap = UMAP(n_components=3, init='random', random_state=0, metric=\"cosine\")\n",
    "    # umap = UMAP(n_components=2, init='random', random_state=0, metric=\"cosine\") # 2D\n",
    "    umap_results = umap.fit_transform(df_subset)\n",
    "    # np.save(\"umap/layer\" + str(layer) + \"_head\" + str(head) + \".npy\", umap_results) # save umap results too\n",
    "    print('UMAP done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "    \n",
    "    return umap_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e6d0df8-eeae-4db7-8526-dda111125af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT GENERATION\n",
    "# add additional columns to df\n",
    "def add_to_df(df, half, attn_dict):\n",
    "    # positions (not normalized)\n",
    "    df['pos_int'] = attn_dict['positions'][:half] * 2\n",
    "    df['pos_int'] = df['pos_int'] + 1\n",
    "    \n",
    "    # length of sentence\n",
    "    words = df['sentence'].str.split().str.len()\n",
    "    df['length'] = words\n",
    "    \n",
    "    # corresponding color for queries/keys\n",
    "    colors = []\n",
    "    for t in df['type']:\n",
    "        if t == \"query\":\n",
    "            colors.append(\"#B6E1B9\")\n",
    "        else:\n",
    "            colors.append(\"#F6BA98\")\n",
    "    df['color'] = colors\n",
    "    df['norm'] = round(df['norm'], 2)\n",
    "\n",
    "    return df\n",
    "\n",
    "def make_fig(tsne_results, df, layer, head, plot_type, half):\n",
    "    # plot TSNE / UMAP results with plotly\n",
    "    # 3D version\n",
    "    # fig = px.scatter_3d(\n",
    "    #     tsne_results[half:], x=0, y=1, z=2,\n",
    "    #     color=df.norm[half:], labels={'color': 'normalized position'}, color_continuous_scale=px.colors.sequential.Burgyl,\n",
    "    #     title=plot_type + ' Plot for BERT (Layer ' + str(layer) + ', Head ' + str(head) + ')',\n",
    "    #     height=800,\n",
    "    #     opacity=0.5\n",
    "    # )\n",
    "    \n",
    "    # 2D version\n",
    "    fig = px.scatter(\n",
    "        tsne_results[half:], x=0, y=1,\n",
    "        color=df.position[half:], labels={'color': 'normalized position'}, color_continuous_scale=px.colors.sequential.Burgyl,\n",
    "        title=plot_type + ' Plot for GPT (Layer ' + str(layer) + ', Head ' + str(head) + ')',\n",
    "        height=800,\n",
    "        opacity=0.5\n",
    "    )\n",
    "    \n",
    "    # 3D version\n",
    "    # fig2 = px.scatter_3d(\n",
    "    #     tsne_results[:half], x=0, y=1, z=2, \n",
    "    #     color=df.position[:half], labels={'color': ''}, color_continuous_scale=px.colors.sequential.Blugrn,\n",
    "    #     title=plot_type + ' Plot for BERT (Layer ' + str(layer) + ', Head ' + str(head) + ')',\n",
    "    #     height=800,\n",
    "    #     opacity=0.5\n",
    "    # )\n",
    "    fig2 = px.scatter(\n",
    "        tsne_results[:half], x=0, y=1, \n",
    "        color=df.position[:half], labels={'color': ''}, color_continuous_scale=px.colors.sequential.Blugrn,\n",
    "        title=plot_type + ' Plot for GPT (Layer ' + str(layer) + ', Head ' + str(head) + ')',\n",
    "        height=800,\n",
    "        opacity=0.5\n",
    "    )\n",
    "    \n",
    "    # add second trace to include 2 color scales (1st is key, 2nd is query)\n",
    "    fig.layout.coloraxis2 = fig2.layout.coloraxis\n",
    "    fig.add_trace(fig2.data[0])\n",
    "    fig['data'][1]['marker'] = {    'color' : df['position'][:half],\n",
    "                                    'coloraxis' : 'coloraxis2',\n",
    "                                    'opacity' : 0.5\n",
    "                                }\n",
    "    # formatting things\n",
    "    fig.layout.coloraxis.colorbar.x = 1.05\n",
    "    fig.layout.coloraxis.colorbar.title.side = \"right\"\n",
    "    fig.layout.coloraxis2.colorbar.x = 1.01\n",
    "    fig.layout.coloraxis2.colorbar.ticklabelstep=70\n",
    "    fig.layout.coloraxis2.colorbar.ticklabelposition=\"inside\"\n",
    "    \n",
    "    # updating display\n",
    "    fig.update_traces( # queries\n",
    "        customdata=df[['token', 'sentence', 'pos_int', 'length', 'type', 'color', 'norm']][:half],\n",
    "        hovertemplate=\"<b style='font-size:larger'><span style='color:%{customdata[5]}'>%{customdata[0]}</span> (<i>%{customdata[4]}</i>, pos: %{customdata[2]} of %{customdata[3]}, norm: %{customdata[6]})</b><br><br>%{customdata[1]}\",\n",
    "        selector=dict(marker_coloraxis='coloraxis2'),\n",
    "        marker=dict(size=6)\n",
    "    )\n",
    "    fig.update_traces( # keys\n",
    "        customdata=df[['token', 'sentence', 'pos_int', 'length', 'type', 'color', 'norm']][half:],\n",
    "        hovertemplate=\"<b style='font-size:larger'><span style='color:%{customdata[5]}'>%{customdata[0]}</span> (<i>%{customdata[4]}</i>, pos: %{customdata[2]} of %{customdata[3]}, norm: %{customdata[6]})</b><br><br>%{customdata[1]}\",\n",
    "        selector=dict(marker_coloraxis='coloraxis'),\n",
    "        marker=dict(size=6)\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='#E8E8E8',\n",
    "        hoverlabel=dict(font_color = 'white', bordercolor = 'white'),\n",
    "    )\n",
    "    \n",
    "    # save plot as html file\n",
    "    # fig.write_html(plot_type + \"_plots/layer\" + str(layer) + \"_head\" + str(head) + \".html\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a1ef8a5-8304-4289-bcc1-dafd60f86335",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FULL TSNE/UMAP LOOPS\n",
    "# generate tsne plot for specific layer, head\n",
    "const = 2\n",
    "def generate_tsne(layer, head):\n",
    "    df = make_df(layer, head)\n",
    "    df = translate_loop(df)\n",
    "    tsne_results = run_tsne(df, layer, head)\n",
    "    half = int(len(tsne_results) / const)\n",
    "    df = add_to_df(df, half)\n",
    "    make_fig(tsne_results, df, layer, head, \"TSNE\", half)\n",
    "\n",
    "# generate umap plot for specific layer, head\n",
    "def generate_umap(layer, head, attn_dict, scaling_const):\n",
    "    df = make_df(layer, head, attn_dict, scaling_const)\n",
    "    df = translate_loop(df)\n",
    "    umap_results = run_umap(df, layer, head)\n",
    "    half = int(len(umap_results) / const)\n",
    "    df = add_to_df(df, half, attn_dict)\n",
    "    make_fig(umap_results, df, layer, head, \"UMAP\", half)\n",
    "    \n",
    "# generate tsne & umap simultaneously\n",
    "def generate_tsne_and_umap(layer, head):\n",
    "    df = make_df(layer, head)\n",
    "    df = translate_loop(df)\n",
    "    tsne_results = run_tsne(df, layer, head)\n",
    "    umap_results = run_umap(df, layer, head)\n",
    "    half = int(len(tsne_results) / const)\n",
    "    df = add_to_df(df, half)\n",
    "    make_fig(tsne_results, df, layer, head, \"TSNE\", half)\n",
    "    make_fig(umap_results, df, layer, head, \"UMAP\", half)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f219da-6f2c-4859-a401-98395c641e9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### plot generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f05ce332-f90a-4661-97ac-05b32d6f51e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer = 5\n",
    "head = 3\n",
    "\n",
    "# ones to look at l1,h2 l1,h3\n",
    "\n",
    "# generate single tsne OR umap plot by itself\n",
    "# generate_tsne(layer, head, attn_dict_small)\n",
    "consts = [1/3, 1/2, 1, 2]\n",
    "for c in consts:\n",
    "    generate_umap(layer, head, attn_dict, c)\n",
    "\n",
    "# generate single tsne AND umap plot\n",
    "# generate_tsne_and_umap(layer, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016548a-1c5d-4570-94e9-522da6a051a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for generating plots\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        generate_tsne_and_umap(i, j)\n",
    "        print(\"Layer {} Head {} done\".format(i, j))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research23",
   "language": "python",
   "name": "research23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
